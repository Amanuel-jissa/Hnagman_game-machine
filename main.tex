\documentclass{article} \usepackage{amsmath} \usepackage{geometry} \geometry{a4paper, margin=1in} \title{Explanation/description} \author{Amanuel Jissa} \date{\today} \begin{document} \maketitle  
 
The Python script that I worked on approaches the challenge not from learning solely from playing the game but from the model being trained to mimic the output of an "oracle". The Oracle has access to the entire dictionary and can calculate the probability distribution for the next best guess in any given state of the game. \\


The model defines the \texttt{ConvOracleNet} class as a network designed to process several features representing the state of the game. Let the state at turn $t$ be $S_t$. $S_t$ is represented by a set of features, The mask $M_t$, the set of guessed letters $G_t$, the remaining lives $L_t$, and a vector of static features $X_t$.


\begin{enumerate}
    \item CNNs  This NN helps the model capture information that is suited to answer questions like "When I see the letters $'t', 'h', '\_'$ next to each other in that order, what is the most likely character for the blank?". I'm more familiar with the application of CNN in a 2D image, for instance a NN might learn to recognize a specific part/section of an image by finding a specific positional arrangement of pixels like a small dark circle (pupil) inside a larger light circle (iris). The filter only cares about that local pattern of pixels. However, in the Hangman AI, i considered the space as the 1D sequence of the word mask. The CNN's Conv1d layer slides a small window which might be 3, 3 or 5 across this sequence. The filter is a learned set of weights that activates when it faces a specific and recognizable pattern of letters and blanks.
     \item Bi-LSTM:- i used this to process the word mask
    as a sequence, capturing contextual and long-range dependencies between characters.
    First the word mask is converted into a sequence of integer indices which are then passed through an embedding layer to create dense vector representation for each character.
    To do this I cconstructed a multi-layer bidirectional LSTM that processes the embedded sequence. This was in hopes of the model allowing the network to gather information from both left-to-right and right-to-left contexts for every position in the sequence. The final hidden states from the forward and backward passes of the last layer are concatenated to form the LSTM feature vector.
    \item GCNs:- I implemented this tool to model the intrinsic relationships between characters in the English language. after a graph is constructed where characters are nodes and edges are weighted by co-occurrence frequency, the code then learns from the structural properties of the language itself.
    The code is trained on a dataset of game states that are generated through simulations against an oracle. This oracle calculates the true character frequency distribution from all possible words consistent with the current game state. The model's loss function is a combination of standard cross-entropy and a KL divergence term which encourages the model's output distribution to align with the oracle's. 
    
\end{enumerate}
The GCN needs a little more explanation.
in the scropt, the \texttt{GraphConvolutionalOperator} class is a PyTorch implementation of a two-layer Graph Convolutional Network (GCN). 

the class is useful to refine the features of each node in a graph by sharing information from its neighbors. In the context of the Hangman AI the nodes represent the letters of the alphabet, and the graph structure or the adjacency matrix tells us the relationships between them, such as how often they co-occur in English words. This operator takes a set of initial node features and the graph's adjacency matrix as input. It then produces a single, fixed-size vector that summarizes the entire graph's state after propagating information between the letters.

The main GCN layer is its propagation rule which mathematically defines how node features are updated. The canonical representations for a single GCN layer is: $$ H^{(l+1)} = \sigma(\hat{A} H^{(l)} W^{(l)}) $$
where $H^{(l)}$ is the matrix of node features at layer $l$. Each row corresponds to a node, and each column is a feature. For the first layer, this is the initial input feature matrix, often denoted as $X$. $W^{(l)}$ is the learnable weight matrix for layer $l$. This is the primary parameter that the model learns during training. Its role is to apply a linear transformation to the node features, projecting them into a different feature space. $\hat{A}$ is the normalized adjacency matrix of the graph. This matrix encodes the graph's structure. The element $\hat{A}_{ij}$ is non-zero if node $i$ and node $j$ are connected. The normalization (often symmetric) is crucial for stabilizing the learning process. $\sigma$ is a non-linear activation function, such as ReLU ($\text{max}(0, x)$), applied element-wise. $H^{(l+1)}$ is the matrix of node features for the next layer, which serves as the output of the current layer.

to beter understad this we can consider the product $H^{(l)} W^{(l)}$ as a feture applying a learned linear transformation to the features of every node independently. In the code beow we achieve this by the nn.Linear layers. then the product $\hat{A} (\dots)$ performs the "convolution". For each node, it computes a weighted sum of the feature vectors of its neighbors (as defined by $\hat{A}$). This step smooths features across the graph.\\





To connect the formula to the code, we start by mapping the formula directly to the \texttt{forward} method of the \texttt{GraphConvolutionalOperator}. 
\begin{enumerate}
    \item First Layer of Propagation: The code for the first GCN layer is: 
    \begin{verbatim} transformed_features_1 = self.feature_transform_1(node_features) 
    
    propagated_features_1 = torch.relu(adjacency_matrix @ transformed_features_1) \end{verbatim} 
    
    This directly implements the formula $H^{(1)} = \sigma(A H^{(0)} W^{(0)})$: 
    \begin{enumerate}
     \item \texttt{node\_features} is the input feature matrix $H^{(0)}$ (also denoted as $X$). \item \texttt{self.feature\_transform\_1} is the learnable weight matrix $W^{(0)}$. \item The line \texttt{transformed\_features\_1 = ...} computes the product $H^{(0)}W^{(0)}$. \item \texttt{adjacency\_matrix} is the adjacency matrix $A$. \item The line \texttt{propagated\_features\_1 = ...} computes the full expression $A(H^{(0)}W^{(0)})$ via the matrix multiplication operator \texttt{@}, and then applies the ReLU activation function $\sigma$. The result is the new set of node features, $H^{(1)}$. 
    \end{enumerate}
    \item Second Layer of Propagation: The code for the second layer is: 
    \begin{verbatim} transformed_features_2 = self.feature_transform_2(propagated_features_1) 
 
    propagated_features_2 = adjacency_matrix @ transformed_features_2 \end{verbatim} 
    
    This implements the formula for the next layer, $H^{(2)} = A H^{(1)} W^{(1)}$:
    \begin{enumerate}
     \item \texttt{propagated\_features\_1} is the feature matrix from the previous layer, $H^{(1)}$. \item \texttt{self.feature\_transform\_2} is the second weight matrix, $W^{(1)}$. \item The code computes the product $H^{(1)}W^{(1)}$ and then aggregates neighborhood information by multiplying with $A$. Note that a non-linear activation is often omitted on the final layer of a GCN block. 


    \end{enumerate}
\end{enumerate}







The final line is: \begin{verbatim} return propagated_features_2.mean(dim=1) \end{verbatim} what this does is it performs a global mean pooling operation. After two layers of propagation, \texttt{propagated\_features\_2} is a matrix of size $[\text{num\_nodes} \times \text{output\_dim}]$. This step averages the features of all nodes to produce a single vector of size $[\text{output\_dim}]$.\end{document}