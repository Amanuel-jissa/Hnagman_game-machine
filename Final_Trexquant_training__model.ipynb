{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1zKnr2GQJJrQIbG1M9jDQW5xWVhjxuGp7",
      "authorship_tag": "ABX9TyNkqXJNK0rnowAdWTiQo3SD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanuel-jissa/Hnagman_game-machine/blob/main/Final_Trexquant_training__model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A CNN+Bi-LSTM+GCN approch to hangman game player model.\n",
        "\n",
        " The model i have worked on uses an architecture of multiple tools to predict the optimal next character guess.\n",
        " Specifically, the model combines three distinct neural network methods:\n",
        "\n",
        " 1. CNNs:- This NN helps the model capture information that is suited to answer questions like \"When I see the letters 't', 'h', '_' next to each other in that order, what is the most likely character for the blank?\". I'm more familiar with the application of CNN in a 2D image, for instance a NN might learn to recognize a specific part/section of an image by finding a specific positional arrangement of pixels like a small dark circle (pupil) inside a larger light circle (iris). The filter only cares about that local pattern of pixels.\n",
        "\n",
        "  However, in the Hangman AI, i considered the space as the 1D sequence of the word mask. The CNN's Conv1d layer slides a small window which might be 3, 3 or 5 across this sequence. This filter is a learned set of weights that activates when it faces a specific and recognizable pattern of characters and blanks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 2. Bi-LSTM:- i used this to process the word mask\n",
        "    as a sequence, capturing contextual and long-range dependencies between characters.\n",
        "    First the word mask is converted into a sequence of integer indices which are then passed through an embedding layer to create dense vector representation for each character.\n",
        "    To do this I cconstructed a multi-layer bidirectional LSTM that processes the embedded sequence. This was in hopes of the model allowing the network to gather information from both left-to-right and right-to-left contexts for every position in the sequence. The final hidden states from the forward and backward passes of the last layer are concatenated to form the LSTM feature vector.\n",
        "\n",
        "\n",
        "\n",
        " 3. GCNs:- I implemented this tool to model the intrinsic relationships between\n",
        "    characters in the English language. after a graph is constructed where characters are nodes\n",
        "    and edges are weighted by co-occurrence frequency, the code then learns\n",
        "    from the structural properties of the language itself.\n",
        "\n",
        "    The code is trained on a dataset of game states that are generated through simulations against an oracle. This oracle calculates the true character frequency distribution from all\n",
        "    possible words consistent with the current game state. The model's loss function is a\n",
        "    combination of standard cross-entropy and a KL divergence term which encourages the model's output distribution to align with the oracle's.\n"
      ],
      "metadata": {
        "id": "JgMAmId3RxuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zfwoJncQecZ"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from functools import lru_cache\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.auto import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURATION"
      ],
      "metadata": {
        "id": "k6fHNQrt9d85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_file_path = \"/content/drive/MyDrive/trained1/sim/words_250000_train.txt\"\n",
        "model_save_directory = \"/content/drive/MyDrive/trained2\"\n",
        "cached_simulation_path = \"/content/drive/MyDrive/trained1/sim/0_77_simulated_game_states.pkl\"\n",
        "Path(model_save_directory).mkdir(exist_ok=True, parents=True)\n",
        "# parameters\n",
        "max_word_len = 50\n",
        "max_lives = 6\n",
        "alphabet = list(string.ascii_lowercase)\n",
        "blank_char = \"_\"\n",
        "# the character vocabulary includes all letters and the blank character\n",
        "char_vocab_size = len(alphabet) + 1\n",
        "life_feature_dim = max_lives + 1 # input dimension for the lives remaining feature (0 to 6)\n",
        "# for the sake of reproducibility and comparing models with different updates.\n",
        "random_seed = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(random_seed); np.random.seed(random_seed); torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(random_seed)\n",
        "# Hyperparameters for the training\n",
        "cnn_layer_1_filters, cnn_layer_2_filters, cnn_layer_3_filters = 1024, 512, 256\n",
        "embedding_dim, lstm_hidden_dim = 256, 512\n",
        "batch_size = 1024\n",
        "total_epochs = 30\n",
        "warmup_lr, max_lr = 2e-4, 2e-3\n",
        "cosine_annealing_period = 24\n",
        "label_smoothing_factor = 0.05\n",
        "gradient_clip_value = 2.0\n",
        "# We gradually introduce the KL divergence loss to stabilize initial training.\n",
        "kl_annealing_decay = 1.0\n",
        "kl_warmup_epochs = 4\n",
        "kl_start_weight, kl_min_weight = 0.21, 0.02\n",
        "# Training strategy\n",
        "early_stopping_patience = 4\n",
        "endgame_oversample_factor = 3  # Re-add states from near-solved games to focus on the endgame\n",
        "hard_word_min_len, hard_word_max_len = 4, 10\n",
        "hard_word_oversample_factor = 4 # Add more mediumlength words, which are often trickier"
      ],
      "metadata": {
        "id": "RFb3SeY59VF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and preparing the dictionary\n",
        "with open(dictionary_file_path) as f:\n",
        "    all_words_from_file = [word.strip().lower() for word in f if word.strip().isalpha()]\n",
        "print(\"loading dictionanry\")\n",
        "# split into a main set for training or simulation and a holdout set for validation(2% of the training set)\n",
        "random.shuffle(all_words_from_file)\n",
        "split_index = int(0.98 * len(all_words_from_file))\n",
        "main_word_list = all_words_from_file[:split_index]\n",
        "holdout_word_list = all_words_from_file[split_index:]\n",
        "# this potentially will group words by length for fasterlookups by the oracle\n",
        "words_by_length = defaultdict(list)\n",
        "for word in main_word_list:\n",
        "    words_by_length[len(word)].append(word)\n",
        "\n",
        "#coverts word lists to numpy character arrays to increase speed\n",
        "char_arrays_by_length = {\n",
        "    length: np.array([list(word) for word in word_list], dtype=\"<U1\")\n",
        "    for length, word_list in words_by_length.items() if length <= max_word_len\n",
        "}\n",
        "\n",
        "# calculate positional letter frequencies\n",
        "positional_char_freqs = np.zeros((max_word_len, len(alphabet)), np.float32)\n",
        "position_counts = np.zeros(max_word_len)\n",
        "for word in main_word_list:\n",
        "    for i, char in enumerate(word[:max_word_len]):\n",
        "        positional_char_freqs[i, alphabet.index(char)] += 1\n",
        "        position_counts[i] += 1\n",
        "# normalizing to get probabilities\n",
        "positional_char_freqs /= position_counts[:, None] + 1e-8 # add small epsilon to avoid division by zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBav_qK69VAb",
        "outputId": "2849f5dc-0550-4f9c-a2ef-9930833c5aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dictionanry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build graph based features\n",
        "char_to_index = {c: i for i, c in enumerate(alphabet)}\n",
        "num_chars = len(alphabet)\n",
        "# build a graph where nodes are letters and edges represent co-occurrence in words\n",
        "cooccurrence_counts = np.zeros((num_chars, num_chars), dtype=np.uint32)\n",
        "for word in main_word_list:\n",
        "    # for every pair of unique letters in a word increment their co-occurrence count\n",
        "    for char1, char2 in combinations(set(word), 2):\n",
        "        idx1, idx2 = char_to_index[char1], char_to_index[char2]\n",
        "        cooccurrence_counts[idx1, idx2] += 1\n",
        "        cooccurrence_counts[idx2, idx1] += 1 # symmetric relationship\n",
        "\n",
        "# normalize to create a transitionlike matrix\n",
        "cooc_matrix = cooccurrence_counts.astype(np.float32)\n",
        "cooc_matrix /= cooc_matrix.sum(axis=1, keepdims=True) + 1e-9\n"
      ],
      "metadata": {
        "id": "okM4nxsQ9U-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  this script analyzes a given dictionary file to recommend a data-driven\n",
        "#  alpha (damping factor) for the PageRank algorithm.\n",
        "#\n",
        "#  It does thid by\n",
        "#  1. Building a character co-occurrence graph from the dictionary.\n",
        "#  2. Analyzing the graph's structural properties (density and clustering).\n",
        "#  3. Applying heuristics to adjust a baseline alpha, making it tailored\n",
        "#     to the specific structure of the language in the dictionary.\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import string\n",
        "from pathlib import Path\n",
        "def analyze_dictionary_and_recommend_alpha(file_path: str) -> float:\n",
        "    \"\"\"\n",
        "    Reads a dictionary, builds a character graph, and recommends a PageRank alpha.\n",
        "    args:\n",
        "        file_path: The full path to the dictionary text file.\n",
        "    returns:\n",
        "        The recommended alpha value as a float.\n",
        "    \"\"\"\n",
        "    with open(file_path) as f:\n",
        "        # Read words, ensuring they are clean and lowercase\n",
        "        words = [word.strip().lower() for word in f if word.strip().isalpha()]\n",
        "\n",
        "\n",
        "    character_graph = nx.from_numpy_array(cooccurrence_counts, create_using=nx.Graph)\n",
        "\n",
        "    density = nx.density(character_graph)\n",
        "    print(f\"Graph Density is {density:.4f}\")\n",
        "\n",
        "    # Metric 2: Average Clustering Coefficient\n",
        "    # Measures the \"cliqueness\" of the graph. High clustering means that letters\n",
        "    # that are connected to the same letter are also likely connected to each other.\n",
        "    # We use the 'weight' parameter to consider the strength of connections.\n",
        "    avg_clustering = nx.average_clustering(character_graph, weight='weight')\n",
        "    print(f\"Average Clustering Coefficient: {avg_clustering:.4f}\")\n",
        "    # Heuristic for Density:\n",
        "    # If the graph is very dense, the links are less meaningful, so we should\n",
        "    # rely more on random \"teleportation\" (i.e., a lower alpha).\n",
        "    # If the graph is sparse, links are rare and important, so we should follow\n",
        "    # them more often (i.e., a higher alpha).\n",
        "    # NOTE: These thresholds are examples and can be tuned.\n",
        "    base_alpha=0.85\n",
        "    if density > 0.85:\n",
        "        print(\"Heuristic Applied: Graph is very dense. Lowering alpha.\")\n",
        "        base_alpha -= 0.05\n",
        "    elif density < 0.50:\n",
        "        print(\"Heuristic Applied: Graph is sparse. Increasing alpha.\")\n",
        "        base_alpha += 0.05\n",
        "\n",
        "    # Heuristic for Clustering:\n",
        "    # If the graph is highly clustered, it has meaningful communities of letters\n",
        "    # (like 's', 't', 'r'). We want to encourage the algorithm to explore these\n",
        "    # local communities, so we increase alpha.\n",
        "    # NOTE: This threshold is an example and can be tuned.\n",
        "    if avg_clustering > 0.65:\n",
        "        print(\"Heuristic Applied: Graph is highly clustered. Increasing alpha.\")\n",
        "        base_alpha += 0.05\n",
        "\n",
        "    # --- Step 5: Finalize and Return the Recommended Value ---\n",
        "    # Ensure the final alpha stays within a reasonable, stable range.\n",
        "    final_alpha = max(0.70, min(0.99, 0.85))\n",
        "\n",
        "\n",
        "    print(f\"recommended Alpha value iss: {final_alpha:.4f}\")\n",
        "\n",
        "\n",
        "    return final_alpha\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recommended_alpha = analyze_dictionary_and_recommend_alpha(dictionary_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrpPQQ4m9Uzq",
        "outputId": "375775f3-0c32-4abc-91b6-4722cceb2f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Density is 1.0000\n",
            "Average Clustering Coefficient: 0.1567\n",
            "Heuristic Applied: Graph is very dense. Lowering alpha.\n",
            "recommended Alpha value iss: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as expected graph density is 1.0 which is saying that the character co-occurrence graph is fully connected or as we call it in math, its a complete graph.\n",
        "\n",
        "In the context of this project, this would mean that for every possible pair of the 26 letters in the alphabet, there is at least one word in the dictionary file that contains both of those letters.\n",
        "\n",
        "the clustering coefficient is a measure of how \"cliquey\" a network is where an average clustering value of 0 indicates low clustering and 1 indicates high clustering. an average clustering of 15.67% indicate that its low. we can understanf this by looking at the following example, for instance if a letter 'C' is frequently found in words with two other letters like 'H' and 'T', there is only a 15.7% chance that 'H' and 'T' are also strongly connected to each other."
      ],
      "metadata": {
        "id": "DhtIcfrEzsdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"this part of the code is converting the raw data on letter pairs into a network.\n",
        "it does this so that the model can apply advanced graph algorithms to measure the strategic\n",
        "importance of each letter, which we will then use to help the model make smarter guesses.\n",
        "I gOt the idea from https://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html \"\"\"\n",
        "character_graph = nx.from_numpy_array(cooccurrence_counts, create_using=nx.Graph)\n",
        "# ffeature 1- pagerank - identifies generally important letters\n",
        "pagerank_scores = nx.pagerank(character_graph, alpha=0.8, weight=\"weight\")\n",
        "pagerank_vector = np.array([pagerank_scores[i] for i in range(num_chars)], dtype=np.float32)\n",
        "# feature 2- personalized pageRank - measures proximity of all letters to a given letter\n",
        "# we pre-calculate the PPR vector for each possible starting letter.\n",
        "def calculate_personalized_pagerank(seed_char_index, alpha=0.85):\n",
        "    personalization_vector = {i: 0.0 for i in range(num_chars)}\n",
        "    personalization_vector[seed_char_index] = 1.0\n",
        "    ppr = nx.pagerank(character_graph, alpha=alpha, weight=\"weight\", personalization=personalization_vector)\n",
        "    return np.array([ppr[i] for i in range(num_chars)], dtype=np.float32)\n",
        "\n",
        "ppr_matrix = np.stack([calculate_personalized_pagerank(i) for i in range(num_chars)], axis=1)\n",
        "@lru_cache(maxsize=None)\n",
        "def build_game_state_features(mask: str):\n",
        "    \"\"\"creates a feature vector from the current game mask and revealed letters.\"\"\"\n",
        "    padded_mask = mask.ljust(max_word_len, blank_char)[:max_word_len]\n",
        "\n",
        "    # feature Set 1- Positional character frequencies for all blank spots\n",
        "    pos_freq_features = [\n",
        "        positional_char_freqs[i, j] if padded_mask[i] == blank_char else 0.0\n",
        "        for i in range(max_word_len) for j in range(len(alphabet))\n",
        "    ]\n",
        "    base_features = np.array(pos_freq_features, np.float32)\n",
        "\n",
        "    # feature Set 2- Graph-based features derived from revealed letters\n",
        "    revealed_chars = [char for char in padded_mask if char != blank_char]\n",
        "    if revealed_chars:\n",
        "        revealed_indices = [char_to_index[ch] for ch in revealed_chars]\n",
        "        # Sum of co-occurrence scores with all revealed letters\n",
        "        cooc_sum_features = cooc_matrix[:, revealed_indices].sum(axis=1)\n",
        "        # average ppr from the perspective of each revealed letter\n",
        "        ppr_avg_features = ppr_matrix[:, revealed_indices].mean(axis=1)\n",
        "    else:\n",
        "        # if no letters are revealed, these features are zero\n",
        "        cooc_sum_features = np.zeros(num_chars, np.float32)\n",
        "        ppr_avg_features = np.zeros(num_chars, np.float32)\n",
        "\n",
        "    # Z-score normalization helps stabilize the features\n",
        "    def z_normalize(v):\n",
        "        return (v - v.mean()) / (v.std() + 1e-6)\n",
        "\n",
        "    # concatenate all features into a single vector\n",
        "    graph_features = np.concatenate([pagerank_vector, z_normalize(cooc_sum_features), z_normalize(ppr_avg_features)], axis=0)\n",
        "    return np.concatenate([base_features, graph_features], axis=0)\n",
        "\n",
        "# we define the feature dimension dynamically from a sample call\n",
        "feature_vector_dim = len(build_game_state_features(blank_char * max_word_len))\n",
        "\n",
        "print(\"the dimension of the feature is=\", feature_vector_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVOjJZ5Y9U76",
        "outputId": "16366721-3e86-49b2-9809-853dc2fde64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the dimension of the feature is= 1378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=16384)\n",
        "def oracle(mask: str, guessed_letters: frozenset):\n",
        "    \"\"\"\n",
        "    this function calculates the ground-truth probability distribution of the next\n",
        "    correct letter, given the current state, by looking at the entire dictionary.\n",
        "    we use thhis to generate the better training targets for the model.\n",
        "    \"\"\"\n",
        "    word_len = len(mask)\n",
        "\n",
        "    # in case we dont have any words of this length in our dictionary we can't make an\n",
        "    # informed guess so it returns a uniform probability distribution over all letters.\n",
        "    # this safety case is more useful when we consider small subsets of the dictionary for\n",
        "    # experimenting on better hyperparameters.\n",
        "    if word_len not in char_arrays_by_length:\n",
        "        return np.full(len(ALPHABET), 1 / len(ALPHABET), np.float32)\n",
        "\n",
        "    # Retrieves the pre-computed numpy array of all words of the correct length.\n",
        "    candidate_words = char_arrays_by_length[word_len]\n",
        "\n",
        "    # create a boolean array, initially all true, to mark which words are still valid candidates.\n",
        "    is_valid = np.ones(len(candidate_words), bool)\n",
        "    guessed_list = list(guessed_letters)\n",
        "\n",
        "    # loop through each character of the current mask\n",
        "    for i, char in enumerate(mask):\n",
        "        # if the character is a revealed letter (not a blank).\n",
        "        if char != BLANK:\n",
        "            # A valid candidate word MUST have this same letter at this exact position.\n",
        "            # I used '&=' to progressively eliminate words that don't match.\n",
        "            is_valid &= (candidate_words[:, i] == char)\n",
        "        # if the character is a blank, and we have guessed some letters.\n",
        "        elif guessed_list:\n",
        "            # the letter at this blank position can't be one of the previously guessed letters\n",
        "            # (since those guesses were either wrong or are already revealed elsewhere).\n",
        "            is_valid &= ~np.isin(candidate_words[:, i], guessed_list)\n",
        "    valid_candidates = candidate_words[is_valid]\n",
        "\n",
        "    # if our filtering rules have eliminated all possible words, return a uniform distribution.\n",
        "    if valid_candidates.size == 0:\n",
        "        return np.full(len(ALPHABET), 1 / len(ALPHABET), np.float32)\n",
        "\n",
        "    # count letter frequencies in the remaining valid words\n",
        "\n",
        "    # flaten the 2D array of candidate words into a single 1D array of all possible letters.\n",
        "    flat_chars = valid_candidates.ravel()\n",
        "    unguessed_chars = flat_chars[~np.isin(flat_chars, guessed_list)]\n",
        "\n",
        "    # if there are no unguessed letters left in any of the candidate words, we can't proceed.\n",
        "    if unguessed_chars.size == 0:\n",
        "        return np.full(len(ALPHABET), 1 / len(ALPHABET), np.float32)\n",
        "\n",
        "    # calculate the final probability distribution ---\n",
        "    char_counts = np.zeros(len(ALPHABET), np.float32)\n",
        "    unique_chars, counts = np.unique(unguessed_chars, return_counts=True)\n",
        "    for char, count in zip(unique_chars, counts):\n",
        "        char_counts[ALPHABET.index(char)] = count\n",
        "    # this gives us the probability of each letter being the correct next guess.\n",
        "    return char_counts / char_counts.sum()"
      ],
      "metadata": {
        "id": "RY5mBU_V9U44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function eblow is basically a \"game simulator\" whose only job is to create a good quality dataset for the initial training of the model. It plays thousands of games against itself with the help of the oracle. to do this the function loops through a list of words. For each word, it resets the game state (no letters guessed, no wrong guesses). The while loop continues until the game is won or lost. At every turn, the simulator calls the oracle function. The oracle looks at the entire dictionary and calculates the perfect, ground truth probability distribution for the next guess. so for example, it might determine that 'e' has a 12% chance of being correct, 's' has a 9% chance, and so on. but the simulator always choses the single best letter from the oracle so it would create a very boring and narrow training set. The model would only learn about the perfect path and would be confused if it ever encountered a slightly suboptimal state which is why i used a parameter called temperature to control how much we either stick to the oracle's advice or explore other options(high temperature).\n",
        "so if temperature is low like 0.2, then 1 / temperature is a large number 5. Raising the probabilities to the 5th power will make the highest probability muchlarger than the others. and causes the simulation tow almost always pick the oracle's best guess.\n",
        "But if temperature is high, say 2, then 1 / temperature is a small number (0.5). Taking the square root of the probabilities makes them all closer to each other. This gives unlikely letters a better chance of getting picked."
      ],
      "metadata": {
        "id": "kUQQdmMk89Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(words_to_play: list[str], temperature: float = 0.80):\n",
        "    \"\"\"\n",
        "    function simulates hangman games to generate training samples.\n",
        "    it uses the oracle to make probabilistic guesses, creating a dataset\n",
        "    of tuples.\n",
        "    \"\"\"\n",
        "    training_samples = []\n",
        "    for word in tqdm(words_to_play, desc=\"simulating gameplay\", leave=False):\n",
        "        secret_word = word[:max_word_len]\n",
        "        guessed_so_far, wrong_guesses = set(), 0\n",
        "\n",
        "        while wrong_guesses < max_lives:\n",
        "            # create the current mask\n",
        "            mask = \"\".join(c if c in guessed_so_far else blank_char for c in secret_word)\n",
        "            if blank_char not in mask:\n",
        "                break # when word is solved\n",
        "\n",
        "            # get the better probability distribution from the oracle\n",
        "            oracle_probabilities = oracle(mask, frozenset(guessed_so_far)).copy()\n",
        "\n",
        "            # to generate varied gameplay, we sample from the oracle's distribution\n",
        "            # instead of always picking the best letter.\n",
        "            sampling_probs = oracle_probabilities.copy()\n",
        "            for g in guessed_so_far: # Can't guess the same letter twice\n",
        "                sampling_probs[alphabet.index(g)] = 0\n",
        "            if sampling_probs.sum() < 1e-8:\n",
        "                break\n",
        "            sampling_probs = np.power(sampling_probs + 1e-9, 1 / temperature)\n",
        "            sampling_probs /= sampling_probs.sum()\n",
        "\n",
        "\n",
        "            chosen_guess = np.random.choice(alphabet, p=sampling_probs)\n",
        "            lives_remaining = max_lives - wrong_guesses\n",
        "\n",
        "            training_samples.append((\n",
        "                mask,\n",
        "                guessed_so_far.copy(),\n",
        "                chosen_guess,\n",
        "                oracle_probabilities,\n",
        "                build_game_state_features(mask),\n",
        "                lives_remaining\n",
        "            ))\n",
        "            guessed_so_far.add(chosen_guess)\n",
        "            if chosen_guess not in secret_word:\n",
        "                wrong_guesses += 1\n",
        "    # this part of the code is added later after testing the model. i collected data on which word length\n",
        "    #that the model underperforms and used that data to oversample games.\n",
        "    endgame_states = [sample for sample in training_samples if sample[0].count(\"_\") <= 2]\n",
        "    training_samples += endgame_states * (endgame_oversample_factor - 1)\n",
        "    random.shuffle(training_samples)\n",
        "    return training_samples"
      ],
      "metadata": {
        "id": "SOyTjBCB9U2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanDataset(Dataset):\n",
        "    \"\"\"simple PyTorch dataset to wrap our list of training samples.\"\"\"\n",
        "    def __init__(self, experiences):\n",
        "        self.experiences = experiences\n",
        "    def __len__(self):\n",
        "        return len(self.experiences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.experiences[idx]\n",
        "\n",
        "def create_mask_tensor(masks: list[str]) -> np.ndarray:\n",
        "    \"\"\"converts a batch of string masks into a one hot encoded tensor.\"\"\"\n",
        "    batch_size = len(masks)\n",
        "    local_max_len = min(max_word_len, max(len(m) for m in masks))\n",
        "    tensor = np.zeros((batch_size, char_vocab_size, local_max_len), np.float32)\n",
        "\n",
        "    for i, mask_str in enumerate(masks):\n",
        "        for j, char in enumerate(mask_str[:local_max_len]):\n",
        "            idx = alphabet.index(char) if char in alphabet else char_vocab_size - 1\n",
        "            tensor[i, idx, j] = 1.0\n",
        "    return tensor\n",
        "def create_guessed_letters_tensor(guessed_sets: list[set]) -> np.ndarray:\n",
        "    \"\"\"creates a multi hot tensor indicating which letters have been guessed.\"\"\"\n",
        "    tensor = np.zeros((len(guessed_sets), len(alphabet)), np.float32)\n",
        "    for i, guessed in enumerate(guessed_sets):\n",
        "        for char in guessed:\n",
        "            tensor[i, alphabet.index(char)] = 1.0\n",
        "    return tensor\n",
        "def collate_batch(batch):\n",
        "    \"\"\"processes a batch of samples into tensors for the model.\"\"\"\n",
        "    masks, guessed_sets, chosen_letters, oracle_dists, feature_vectors, lives = zip(*batch)\n",
        "    # padded one hot tensor of the word mask\n",
        "    mask_tensors = torch.from_numpy(create_mask_tensor(masks))\n",
        "    # padded sequence of character indices for the LSTM\n",
        "    seq_len = mask_tensors.shape[2]\n",
        "    mask_indices_list = []\n",
        "    for s in masks:\n",
        "        indices = [alphabet.index(c) if c in alphabet else char_vocab_size - 1 for c in s[:seq_len]]\n",
        "        indices += [char_vocab_size - 1] * (seq_len - len(indices))\n",
        "        mask_indices_list.append(indices)\n",
        "    mask_indices = torch.tensor(mask_indices_list, dtype=torch.long)\n",
        "    # multi hot tensor of guessed letters\n",
        "    guessed_chars_tensor = torch.from_numpy(create_guessed_letters_tensor(guessed_sets))\n",
        "    # stacked tensor of pre computed features\n",
        "    game_state_features = torch.from_numpy(np.stack(feature_vectors))\n",
        "    # onehot tensor for remaining lives\n",
        "    lives_tensor = F.one_hot(torch.tensor(lives), num_classes=life_feature_dim).float()\n",
        "    # target letter indices for the loss function\n",
        "    target_letter_indices = torch.tensor([alphabet.index(letter) for letter in chosen_letters])\n",
        "    # oracle's ground-truth probability distributions\n",
        "    oracle_distributions = torch.from_numpy(np.stack(oracle_dists))\n",
        "    return mask_tensors, mask_indices, guessed_chars_tensor, game_state_features, lives_tensor, target_letter_indices, oracle_distributions\n"
      ],
      "metadata": {
        "id": "QqOZUYtU9UxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided GraphConvolutionalOperator class is a PyTorch implementation of a two layer GCN. Its useful to refine the features of each node in a graph by sharing information from its neighbors. In the context of the Hangman AI, the nodes represent the letters of the alphabet, and the graph structure (the adjacency matrix) tells us the relationships between them, such as how often they co-occur in English words. This operator takes a set of initial node features and the graph's adjacency matrix as input. It then produces a single, fixed-size vector that summarizes the entire graph's state after propagating information between the letters.\n",
        "\n",
        "The main GCN layer is its propagation rule which mathematically defines how node features are updated. The canonical representations for a single GCN layer is: $$ H^{(l+1)} = \\sigma(\\hat{A} H^{(l)} W^{(l)}) $$\n",
        "where $H^{(l)}$ is the matrix of node features at layer $l$. Each row corresponds to a node, and each column is a feature. For the first layer, this is the initial input feature matrix, often denoted as $X$. $W^{(l)}$ is the learnable weight matrix for layer $l$. This is the primary parameter that the model learns during training. Its role is to apply a linear transformation to the node features, projecting them into a different feature space. $\\hat{A}$ is the normalized adjacency matrix of the graph. This matrix encodes the graph's structure. The element $\\hat{A}_{ij}$ is non-zero if node $i$ and node $j$ are connected. The normalization (often symmetric) is crucial for stabilizing the learning process. $\\sigma$ is a non-linear activation function, such as ReLU ($\\text{max}(0, x)$), applied element-wise. $H^{(l+1)}$ is the matrix of node features for the next layer, which serves as the output of the current layer.\n",
        "\n",
        "to beter understad this we can consider the product $H^{(l)} W^{(l)}$ as a feture applying a learned linear transformation to the features of every node independently. In the code beow we achieve this by the nn.Linear layers. then the product $\\hat{A} (\\dots)$ performs the \"convolution\". For each node, it computes a weighted sum of the feature vectors of its neighbors (as defined by $\\hat{A}$). This step smooths features across the graph."
      ],
      "metadata": {
        "id": "ZChsS-t3FxjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model to be used\n",
        "class GraphConvolutionalOperator(nn.Module):\n",
        "    \"\"\"\n",
        "    this is a simple GCN layer. I like to consider this as an operator that transforms\n",
        "    node featuresbased on the graph's structure. It learns a transformation and then\n",
        "    shares the transformed features to neighboring nodes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=128, hidden_dim=128, output_dim=128):\n",
        "        super().__init__()\n",
        "        # These linear layers are the learnable transformations, like W in the GCN formula\n",
        "        self.feature_transform_1 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.feature_transform_2 = nn.Linear(hidden_dim, output_dim, bias=False)\n",
        "    def forward(self, node_features, adjacency_matrix):\n",
        "        \"\"\"\n",
        "        performs the graph convolution operation: o(A*X*W)\n",
        "        where adjacency_matrix is the (A), the graph structure.\n",
        "        and the node_features is the (X), the features at each node\n",
        "        \"\"\"\n",
        "        # First layer of propagation\n",
        "        transformed_features_1 = self.feature_transform_1(node_features)\n",
        "        propagated_features_1 = torch.relu(adjacency_matrix @ transformed_features_1)\n",
        "\n",
        "        # Second layer of propagation\n",
        "        transformed_features_2 = self.feature_transform_2(propagated_features_1)\n",
        "        propagated_features_2 = adjacency_matrix @ transformed_features_2\n",
        "\n",
        "        # We take the mean of all character features to get a single graph representation\n",
        "        return propagated_features_2.mean(dim=1)\n",
        "\n",
        "class HangmanAI_Network(nn.Module):\n",
        "    \"\"\"\n",
        "    The main neural network for guessing letters. It combines several sub-modules:\n",
        "    - An LSTM to understand the sequence of the mask\n",
        "    - A CNN to capture local patterns in the mask.\n",
        "    - A GCN to reason about character relationships.\n",
        "    \"\"\"\n",
        "    def __init__(self, static_feature_dim: int, emb_dim: int, lstm_h_dim: int, gcn_embedding_dim: int):\n",
        "        super().__init__()\n",
        "        #LSTM for sequential mask processing\n",
        "        self.character_embedding = nn.Embedding(char_vocab_size, emb_dim)\n",
        "        self.mask_encoder_lstm = nn.LSTM(emb_dim, lstm_h_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        # CNN for local pattern detection\n",
        "        self.cnn_layer_1 = nn.Conv1d(char_vocab_size, cnn_layer_1_filters, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_layer_1_filters)\n",
        "        self.cnn_layer_2 = nn.Conv1d(cnn_layer_1_filters, cnn_layer_2_filters, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(cnn_layer_2_filters)\n",
        "        self.cnn_layer_3 = nn.Conv1d(cnn_layer_2_filters, cnn_layer_3_filters, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_layer_3_filters)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # GCN for character graph reasoning\n",
        "        self.character_node_embeddings = nn.Parameter(torch.randn(len(alphabet), gcn_embedding_dim))\n",
        "        self.graph_processor = GraphConvolutionalOperator(\n",
        "            input_dim=gcn_embedding_dim, hidden_dim=gcn_embedding_dim, output_dim=gcn_embedding_dim\n",
        "        )\n",
        "        # the adjacency matrix is fixed, so we register it as a non-trainable buffer\n",
        "        adj_matrix = cooc_matrix / (cooc_matrix.sum(1, keepdims=True) + 1e-9)\n",
        "        self.register_buffer(\"adjacency_matrix\", torch.from_numpy(adj_matrix).float())\n",
        "\n",
        "        # final Combination and Output Head\n",
        "        combined_feature_dim = (cnn_layer_3_filters + (2 * lstm_h_dim) + len(alphabet) +\n",
        "                                static_feature_dim + life_feature_dim + gcn_embedding_dim)\n",
        "\n",
        "        self.combiner_head = nn.Sequential(\n",
        "            nn.Linear(combined_feature_dim, 512), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.1)\n",
        "        )\n",
        "        self.output_layer = nn.Linear(256, len(alphabet))\n",
        "\n",
        "    def forward(self, mask_tensors, mask_indices, guessed_chars_tensor, static_features, lives_tensor):\n",
        "        # process mask with LSTM\n",
        "        embedded_sequence = self.character_embedding(mask_indices)\n",
        "        _, (last_hidden_state, _) = self.mask_encoder_lstm(embedded_sequence)\n",
        "        # concatenate the final forward and backward hidden states\n",
        "        lstm_features = torch.cat([last_hidden_state[-2], last_hidden_state[-1]], dim=1)\n",
        "\n",
        "        # process mask with CNN\n",
        "        cnn_out = F.relu(self.bn1(self.cnn_layer_1(mask_tensors)))\n",
        "        cnn_out = F.relu(self.bn2(self.cnn_layer_2(cnn_out)))\n",
        "        cnn_out = F.relu(self.bn3(self.cnn_layer_3(cnn_out)))\n",
        "        cnn_features = self.pool(cnn_out).squeeze(2)\n",
        "        # process character graph with GCN\n",
        "        batch_size = mask_tensors.size(0)\n",
        "        # Expand the character embeddings for each item in the batch\n",
        "        node_features = self.character_node_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        gcn_features = self.graph_processor(node_features, self.adjacency_matrix)\n",
        "\n",
        "        # concatenate all features and predict\n",
        "        full_feature_vector = torch.cat([\n",
        "            cnn_features, lstm_features, guessed_chars_tensor, static_features, lives_tensor, gcn_features\n",
        "        ], dim=1)\n",
        "\n",
        "        combined_output = self.combiner_head(full_feature_vector)\n",
        "        return self.output_layer(combined_output)\n"
      ],
      "metadata": {
        "id": "NsaM0Err9UuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent code\n",
        "class HangmanAgent:\n",
        "    \"\"\"\n",
        "    an advanced agent that uses a hybrid strategy for guessing letters.\n",
        "    - It uses the trained neural network to get a shortlist of promising letters.\n",
        "    - It then uses a typical information gain approach on this shortlist to\n",
        "       make a final decision, which adds robustness.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.device = device\n",
        "        self.alphabet = list(string.ascii_lowercase)\n",
        "        self.model = model\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        # The agent needs access to the dictionary to calculate information gain\n",
        "        self.char_arrays_by_length = char_arrays_by_length\n",
        "    @lru_cache(maxsize=2048)\n",
        "    def find_possible_words(self, mask: str, guessed: frozenset) -> np.ndarray:\n",
        "        #efficiently filters the dictionary to find all words consistent with the current game state.\n",
        "        L = len(mask)\n",
        "        if L not in self.char_arrays_by_length:\n",
        "            return np.empty((0, L), \"<U1\")\n",
        "        arr = self.char_arrays_by_length[L]\n",
        "        is_valid = np.ones(len(arr), bool)\n",
        "        glist = list(guessed)\n",
        "        for i, ch in enumerate(mask):\n",
        "            if ch != blank_char:\n",
        "                is_valid &= (arr[:, i] == ch)\n",
        "            elif glist:\n",
        "                is_valid &= ~np.isin(arr[:, i], glist)\n",
        "        return arr[is_valid]\n",
        "    @torch.no_grad()\n",
        "    def decide_next_guess(self, mask: str, guessed_letters: frozenset, wrong_guesses: int) -> str:\n",
        "        \"\"\"The core decision-making logic of the agent.\"\"\"\n",
        "        # Get predictions from the neural network\n",
        "        mask_tensor = torch.from_numpy(create_mask_tensor([mask])).to(self.device)\n",
        "        mask_len = mask_tensor.shape[2]\n",
        "        indices = [self.alphabet.index(c) if c in self.alphabet else char_vocab_size - 1 for c in mask[:mask_len]]\n",
        "        indices += [char_vocab_size - 1] * (mask_len - len(indices))\n",
        "        mask_indices = torch.tensor([indices], dtype=torch.long, device=self.device)\n",
        "        guessed_tensor = torch.from_numpy(create_guessed_letters_tensor([guessed_letters])).to(self.device)\n",
        "        features_tensor = torch.from_numpy(build_game_state_features(mask)[None, :]).to(self.device)\n",
        "        lives_left = max_lives - wrong_guesses\n",
        "        lives_tensor = F.one_hot(torch.tensor([lives_left], device=self.device), num_classes=life_feature_dim).float()\n",
        "        logits = self.model(mask_tensor, mask_indices, guessed_tensor, features_tensor, lives_tensor)[0]\n",
        "        # mask out already-guessed letters before applying softmax to get a clean distribution\n",
        "        for g in guessed_letters:\n",
        "            if g in self.alphabet:\n",
        "                logits[self.alphabet.index(g)] = -torch.inf\n",
        "        probabilities = F.softmax(logits, dim=0).cpu().numpy()\n",
        "        # Information Gain to re-rank the top candidates\n",
        "        num_candidates_from_model = 7\n",
        "        top_indices = np.argsort(-probabilities)[:num_candidates_from_model]\n",
        "        model_candidates = [self.alphabet[i] for i in top_indices if self.alphabet[i] not in guessed_letters]\n",
        "        if not model_candidates: # Fallback if all top candidates were somehow already guessed\n",
        "            return next(c for c in self.alphabet if c not in guessed_letters)\n",
        "        possible_words = self.find_possible_words(mask, guessed_letters)\n",
        "        num_possible_words = len(possible_words)\n",
        "        # If the word space is very small, just trust the model's best guess.\n",
        "        if num_possible_words <= 1:\n",
        "            return model_candidates[0]\n",
        "        best_letter, min_expected_size = None, float('inf')\n",
        "        # to keep this step fast, we sample from the candidate words if the list is too long.\n",
        "        max_words_for_ig = 1000\n",
        "        if num_possible_words > max_words_for_ig:\n",
        "            sample_indices = np.random.choice(num_possible_words, max_words_for_ig, replace=False)\n",
        "            info_gain_word_sample = possible_words[sample_indices]\n",
        "        else:\n",
        "            info_gain_word_sample = possible_words\n",
        "\n",
        "        sample_size = len(info_gain_word_sample)\n",
        "        for letter in model_candidates:\n",
        "            # we want the guess that best splits the remaining possible words.\n",
        "            # a good method for this is minimizing the expected size of the remaining word list.\n",
        "            hit_count = 0\n",
        "            for word_array in info_gain_word_sample:\n",
        "                # Check if the letter is in any of the unknown slots of the word\n",
        "                if letter in word_array[mask == blank_char]:\n",
        "                    hit_count += 1\n",
        "            p_hit = hit_count / sample_size\n",
        "            # Expected size = P(hit)*(size if hit)+P(miss)*(size if miss)\n",
        "            expected_remaining_size = p_hit * hit_count + (1 - p_hit) * (sample_size - hit_count)\n",
        "            if expected_remaining_size < min_expected_size:\n",
        "                min_expected_size = expected_remaining_size\n",
        "                best_letter = letter\n",
        "        return best_letter if best_letter is not None else model_candidates[0]\n",
        "    def play_game(self, secret_word: str) -> bool:\n",
        "        \"\"\"Plays a single game of Hangman against a given word.\"\"\"\n",
        "        guessed, wrong_guesses = set(), 0\n",
        "        mask = blank_char * len(secret_word)\n",
        "        while blank_char in mask and wrong_guesses < max_lives:\n",
        "            letter_to_guess = self.decide_next_guess(mask, frozenset(guessed), wrong_guesses)\n",
        "            guessed.add(letter_to_guess)\n",
        "            if letter_to_guess not in secret_word:\n",
        "                wrong_guesses += 1\n",
        "            mask = ''.join(c if c in guessed else blank_char for c in secret_word)\n",
        "        return blank_char not in mask # Return True for a win, False for a loss"
      ],
      "metadata": {
        "id": "M2tLASfM9UrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "oDCHsNhFTILe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kl_coefficient(epoch: int) -> float:\n",
        "    \"\"\"warming up schedule for the KL divergence loss weight.\"\"\"\n",
        "    if epoch < kl_warmup_epochs:\n",
        "        return (epoch / kl_warmup_epochs) * kl_start_weight\n",
        "    if epoch < 8:\n",
        "        return kl_start_weight\n",
        "    #after a few epochs, start decaying the weight\n",
        "    return max(kl_start_weight * (kl_annealing_decay ** (epoch - 8)), kl_min_weight)\n",
        "def run_training_loop(model: nn.Module, dataset: Dataset):\n",
        "    # split data into training and validation sets\n",
        "    train_set_size = int(0.9 * len(dataset))\n",
        "    val_set_size = len(dataset) - train_set_size\n",
        "    train_set, validation_set = random_split(\n",
        "        dataset, [train_set_size, val_set_size],\n",
        "        generator=torch.Generator().manual_seed(random_seed)\n",
        "    )\n",
        "    dataloader_config = {'batch_size': batch_size,'collate_fn': collate_batch,'num_workers': 0, 'pin_memory': True}\n",
        "    train_loader = DataLoader(train_set, shuffle=True, **dataloader_config)\n",
        "    validation_loader = DataLoader(validation_set, **dataloader_config)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=1e-5)\n",
        "    scaler = GradScaler() # gor mixed-precision training\n",
        "    # learning rate schedule via cosine\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=warmup_lr / max_lr, end_factor=1.0, total_iters=7)\n",
        "    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=cosine_annealing_period)\n",
        "    scheduler = SequentialLR(optimizer, [warmup_scheduler, cosine_scheduler], milestones=[7])\n",
        "    best_validation_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_path = None\n",
        "    for epoch in range(total_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "\n",
        "        print(f\"\\n {epoch+1}\")\n",
        "        for batch in train_loader:\n",
        "            mask_t, mask_idx, guessed_t, features_t, lives_t, targets, oracle_dist = [t.to(device) for t in batch]\n",
        "\n",
        "            with autocast():\n",
        "                logits = model(mask_t, mask_idx, guessed_t, features_t, lives_t)\n",
        "                # loss is a combination of two objectives:\n",
        "                # - guess the letter correctly (supervised learning).\n",
        "                # -match the Oracle's probability distribution.\n",
        "                ce_loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing_factor)\n",
        "                kld_loss = F.kl_div(F.log_softmax(logits, -1), oracle_dist, reduction='batchmean')\n",
        "                loss = ce_loss + get_kl_coefficient(epoch) * kld_loss\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_train_loss += loss.item() * targets.size(0)\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        total_validation_loss, validation_samples = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_loader:\n",
        "                mask_t, mask_idx, guessed_t, features_t, lives_t, targets, _ = [t.to(device) for t in batch]\n",
        "                with autocast():\n",
        "                    logits = model(mask_t, mask_idx, guessed_t, features_t, lives_t)\n",
        "                    # for validation, we only care about the primary cross entropy loss\n",
        "                    val_loss_batch = F.cross_entropy(logits, targets, reduction='sum')\n",
        "                total_validation_loss += val_loss_batch.item()\n",
        "                validation_samples += targets.size(0)\n",
        "        avg_train_loss = total_train_loss / train_set_size\n",
        "        avg_validation_loss = total_validation_loss / validation_samples\n",
        "        # evaluation on holdout set\n",
        "        # create a temporary agent with the current model state to check win rate\n",
        "        validation_agent = HangmanAgent(model=model, device=device)\n",
        "        num_wins = sum(validation_agent.play_game(w) for w in holdout_word_list[:50])\n",
        "        win_rate = num_wins / 50\n",
        "        print(f\"Ep{epoch}, Val Loss: {avg_validation_loss:.3f}, Holdout WR: {win_rate*100:5.2f}%\")\n",
        "        # model Checkpointing and Early Stopping\n",
        "        if avg_validation_loss < best_validation_loss - 1e-4:\n",
        "            best_validation_loss = avg_validation_loss\n",
        "            epochs_without_improvement = 0\n",
        "            model_path = Path(model_save_directory) / f\"best_model_epoch_{epoch+1}.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            best_model_path = model_path\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= early_stopping_patience:\n",
        "                print(\"Early stoppage\")\n",
        "                break\n",
        "        scheduler.step()\n",
        "    return best_model_path\n"
      ],
      "metadata": {
        "id": "DSSX2laO9Uow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # generate training data\n",
        "    with open(cached_simulation_path, \"rb\") as f:\n",
        "        gameplay_experiences = pickle.load(f)\n",
        "    print(f\"total training samples: {len(gameplay_experiences):,}\")\n",
        "    full_dataset = HangmanDataset(gameplay_experiences)\n",
        "    # startthe model\n",
        "    hangman_model = HangmanAI_Network(static_feature_dim=feature_vector_dim,emb_dim=embedding_dim,lstm_h_dim=lstm_hidden_dim,gcn_embedding_dim=128).to(device)\n",
        "    # start training\n",
        "    best_model_file = run_training_loop(hangman_model, full_dataset)\n",
        "    # final evaluation\n",
        "    if best_model_file and best_model_file.is_file():\n",
        "        print(\"calculating win rate\")\n",
        "        final_model = HangmanAI_Network(static_feature_dim=feature_vector_dim,emb_dim=embedding_dim,lstm_h_dim=lstm_hidden_dim,gcn_embedding_dim=128)\n",
        "        final_model.load_state_dict(torch.load(best_model_file, map_location=device))\n",
        "        # create the final agent with the loaded model\n",
        "        final_agent = HangmanAgent(model=final_model, device=device)\n",
        "        total_wins = 0\n",
        "        for word in holdout_word_list:\n",
        "            if final_agent.play_game(word):\n",
        "                total_wins += 1\n",
        "        final_win_rate = total_wins / len(holdout_word_list)\n",
        "        print(f\"final win rate is: {final_win_rate * 100:.2f}% ({total_wins}/{len(holdout_word_list)})\")\n",
        "    else:\n",
        "        print(\"\\n no model was saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "EivtiSeq9Ul4",
        "outputId": "a34433e5-9164-422b-a6ee-5ba3962a3cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training samples: 8,096,982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-613167103.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() # gor mixed-precision training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-613167103.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-613167103.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep0, Val Loss: 2.276, Holdout WR: 54.00%\n",
            "\n",
            " 2\n",
            "Ep1, Val Loss: 2.223, Holdout WR: 60.00%\n",
            "\n",
            " 3\n",
            "Ep2, Val Loss: 2.188, Holdout WR: 58.00%\n",
            "\n",
            " 4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2444007332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhangman_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHangmanAI_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_feature_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_vector_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_h_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgcn_embedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbest_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhangman_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# final evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_model_file\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbest_model_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-613167103.py\u001b[0m in \u001b[0;36mrun_training_loop\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguessed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlives_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# loss is a combination of two objectives:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# - guess the letter correctly (supervised learning).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-240105311.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mask_tensors, mask_indices, guessed_chars_tensor, static_features, lives_tensor)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# process mask with LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0membedded_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharacter_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_encoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;31m# concatenate the final forward and backward hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlstm_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}